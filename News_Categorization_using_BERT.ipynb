{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "News Categorization using BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMB9c/HxouiYdnuWEY0k0hI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadyadtm/BERT-Implementation-in-News-Categorization/blob/main/News_Categorization_using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWPuIReVvVFi"
      },
      "source": [
        "# News Categorization using BERT\r\n",
        "\r\n",
        "Berikut ini adalah implementasi news categorization (Pengkategorian Berita) dengan menggunakan BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDJPxpG13k6b"
      },
      "source": [
        "## Import Package\r\n",
        "Sebelum memulai implementasi, diperlukan import beberapa package terlebih dahulu dan menginstall package transformer\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6BJDQCiB3a-",
        "outputId": "e1eac771-34ff-481e-99c3-da3907d31932"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!pip install transformers==3\r\n",
        "import transformers\r\n",
        "\r\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from pylab import rcParams\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib import rc\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report\r\n",
        "from collections import defaultdict\r\n",
        "from textwrap import wrap\r\n",
        "from torch import nn, optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from transformers import BertTokenizer\r\n",
        "import os\r\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHD_07WS6SW3"
      },
      "source": [
        "##Set GPU\r\n",
        "Sebelum memulai pelatihan, diperlukan untuk set GPU untuk menjalankan Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FyHj5iP-ycS",
        "outputId": "5d07df8a-6115-4459-b061-1e0e31c0b1c1"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "if torch.cuda.is_available():       \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\r\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "matmQgRv3ny5"
      },
      "source": [
        "## Load dan Split Train dan Test\r\n",
        "Berikut ini adalah langkah untuk load data train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yMbdY2TgEWhv",
        "outputId": "4a49f186-ea28-4373-aad3-11bd128f281e"
      },
      "source": [
        "#Load data train\r\n",
        "df_train = pd.read_csv('drive/My Drive/SMT 2/NLP/Tugas 2/Data_Train.csv',encoding='cp1252')\r\n",
        "df_train.tail()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STORY</th>\n",
              "      <th>SECTION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7623</th>\n",
              "      <td>Karnataka has been a Congress bastion, but it ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>The film, which also features Janhvi Kapoor, w...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7625</th>\n",
              "      <td>The database has been created after bringing t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7626</th>\n",
              "      <td>The state, which has had an uneasy relationshi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7627</th>\n",
              "      <td>Virus stars Kunchacko Boban, Tovino Thomas, In...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  STORY  SECTION\n",
              "7623  Karnataka has been a Congress bastion, but it ...        0\n",
              "7624  The film, which also features Janhvi Kapoor, w...        2\n",
              "7625  The database has been created after bringing t...        1\n",
              "7626  The state, which has had an uneasy relationshi...        0\n",
              "7627  Virus stars Kunchacko Boban, Tovino Thomas, In...        2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVn8BPD-8__d"
      },
      "source": [
        "Kemudian lakukan pembagian data train, validasi, dan data testnya dengan menggunakan lib sklearn. Pembagiannya adalah 90% data train, 5% data validasi, dan 5% data test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNw7WZUt9DFR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X = df_train\r\n",
        "\r\n",
        "X_train, X_val =\\\r\n",
        "    train_test_split(X, test_size=0.1, random_state=2020)\r\n",
        "\r\n",
        "X_val, X_test =\\\r\n",
        "    train_test_split(X_val, test_size=0.5, random_state=2020)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3zuvTQb-EfW",
        "outputId": "bf185a59-7dbf-4a1e-8c9c-3303a2eb08a6"
      },
      "source": [
        "print(\"Jumlah Data Train : \", X_train.shape[0])\r\n",
        "print(\"Jumlah Data Train : \", X_val.shape[0])\r\n",
        "print(\"Jumlah Data Train : \", X_test.shape[0])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jumlah Data Train :  6865\n",
            "Jumlah Data Train :  381\n",
            "Jumlah Data Train :  382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IELt6hLV8QAJ"
      },
      "source": [
        "## Preprocessing\r\n",
        "Preprocessing yang dilakukan pada task ini adalah lowercase dan tokenisasi yang dilakukan oleh package dari BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzgCrNB5JfG"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)\r\n",
        "\r\n",
        "class NewsDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self, stories, targets, tokenizer, max_len):\r\n",
        "    self.stories = stories\r\n",
        "    self.targets = targets\r\n",
        "    self.tokenizer = tokenizer\r\n",
        "    self.max_len = max_len\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.stories)\r\n",
        "  \r\n",
        "  def __getitem__(self, item):\r\n",
        "    story = str(self.stories[item])\r\n",
        "    target = self.targets[item]\r\n",
        "\r\n",
        "    encoding = self.tokenizer.encode_plus(\r\n",
        "      story,\r\n",
        "      add_special_tokens=True,\r\n",
        "      max_length=self.max_len,\r\n",
        "      return_token_type_ids=False,\r\n",
        "      pad_to_max_length=True,\r\n",
        "      truncation=True,\r\n",
        "      return_attention_mask=True,\r\n",
        "      return_tensors='pt'\r\n",
        "    )\r\n",
        "\r\n",
        "    return {\r\n",
        "      'story_text': story,\r\n",
        "      'input_ids': encoding['input_ids'].flatten(),\r\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\r\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\r\n",
        "    }"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UfFJ5FPjvL"
      },
      "source": [
        "Setelah itu membuat data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0NJVMTK1UTg"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\r\n",
        "  ds = NewsDataset(\r\n",
        "    stories=df.STORY.to_numpy(),\r\n",
        "    targets=df.SECTION.to_numpy(),\r\n",
        "    tokenizer=tokenizer,\r\n",
        "    max_len=max_len\r\n",
        "  )\r\n",
        "\r\n",
        "  return DataLoader(\r\n",
        "    ds,\r\n",
        "    batch_size=batch_size,\r\n",
        "    num_workers=4\r\n",
        "  )"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul9U_gfKPntK"
      },
      "source": [
        "Kemudian setting batch size 16 dan MAX_LEN = 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChQFkwIc0RLL"
      },
      "source": [
        "BATCH_SIZE = 16\r\n",
        "MAX_LEN = 30\r\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\r\n",
        "\r\n",
        "train_data_loader = create_data_loader(X_train, tokenizer, MAX_LEN, BATCH_SIZE)\r\n",
        "val_data_loader = create_data_loader(X_val, tokenizer, MAX_LEN, BATCH_SIZE)\r\n",
        "test_data_loader = create_data_loader(X_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCzstg1l3lwz"
      },
      "source": [
        "## Arsitektur Model\r\n",
        "\r\n",
        "Load model BERT dari pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H-4mgb33gwU"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5maazBg8SPCO"
      },
      "source": [
        "Buat arsitektur BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDwtoUq-8Slx"
      },
      "source": [
        "class NewsClassifier(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, n_classes):\r\n",
        "    super(NewsClassifier, self).__init__()\r\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n",
        "    self.drop = nn.Dropout(p=0.3)\r\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n",
        "  \r\n",
        "  def forward(self, input_ids, attention_mask):\r\n",
        "    _, pooled_output = self.bert(\r\n",
        "      input_ids=input_ids,\r\n",
        "      attention_mask=attention_mask\r\n",
        "    )\r\n",
        "    output = self.drop(pooled_output)\r\n",
        "    return self.out(output)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJMlqGmuSRTc"
      },
      "source": [
        "Assign classifier ke GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYmXHjfB8tKL"
      },
      "source": [
        "model = NewsClassifier(4)\r\n",
        "model = model.to(device)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJdURMskST4Z"
      },
      "source": [
        "Set optimizer, loss function, dan scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOy0KAQG864C"
      },
      "source": [
        "EPOCHS = 10\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\r\n",
        "total_steps = len(train_data_loader) * EPOCHS\r\n",
        "\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "  optimizer,\r\n",
        "  num_warmup_steps=0,\r\n",
        "  num_training_steps=total_steps\r\n",
        ")\r\n",
        "\r\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR3M9hnTSeD-"
      },
      "source": [
        "## Proses Pelatihan\r\n",
        "Membuat fungsi train epoch, untuk melakukan training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0657YNGv9IeU"
      },
      "source": [
        "def train_epoch(\r\n",
        "  model, \r\n",
        "  data_loader, \r\n",
        "  loss_fn, \r\n",
        "  optimizer, \r\n",
        "  device, \r\n",
        "  scheduler, \r\n",
        "  n_examples\r\n",
        "):\r\n",
        "  model = model.train()\r\n",
        "\r\n",
        "  losses = []\r\n",
        "  correct_predictions = 0\r\n",
        "  \r\n",
        "  for d in data_loader:\r\n",
        "    input_ids = d[\"input_ids\"].to(device)\r\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "    targets = d[\"targets\"].to(device)\r\n",
        "\r\n",
        "    outputs = model(\r\n",
        "      input_ids=input_ids,\r\n",
        "      attention_mask=attention_mask\r\n",
        "    )\r\n",
        "\r\n",
        "    _, preds = torch.max(outputs, dim=1)\r\n",
        "    loss = loss_fn(outputs, targets)\r\n",
        "\r\n",
        "    correct_predictions += torch.sum(preds == targets)\r\n",
        "    losses.append(loss.item())\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6n4fSNSuXt"
      },
      "source": [
        "Kemudian buat eval model untuk evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMzVKbJS9OS0"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\r\n",
        "  model = model.eval()\r\n",
        "\r\n",
        "  losses = []\r\n",
        "  correct_predictions = 0\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for d in data_loader:\r\n",
        "      input_ids = d[\"input_ids\"].to(device)\r\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "      targets = d[\"targets\"].to(device)\r\n",
        "\r\n",
        "      outputs = model(\r\n",
        "        input_ids=input_ids,\r\n",
        "        attention_mask=attention_mask\r\n",
        "      )\r\n",
        "      _, preds = torch.max(outputs, dim=1)\r\n",
        "\r\n",
        "      loss = loss_fn(outputs, targets)\r\n",
        "\r\n",
        "      correct_predictions += torch.sum(preds == targets)\r\n",
        "      losses.append(loss.item())\r\n",
        "\r\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QECP2Ty2Syc5"
      },
      "source": [
        "Melakukan Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ssmWYe9QzC",
        "outputId": "7a0ab187-8437-471b-f075-0a12d8cd6c0d"
      },
      "source": [
        "history = defaultdict(list)\r\n",
        "best_accuracy = 0\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "\r\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\r\n",
        "  print('-' * 10)\r\n",
        "\r\n",
        "  train_acc, train_loss = train_epoch(\r\n",
        "    model,\r\n",
        "    train_data_loader,    \r\n",
        "    loss_fn, \r\n",
        "    optimizer, \r\n",
        "    device, \r\n",
        "    scheduler, \r\n",
        "    len(X_train)\r\n",
        "  )\r\n",
        "\r\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\r\n",
        "\r\n",
        "  val_acc, val_loss = eval_model(\r\n",
        "    model,\r\n",
        "    val_data_loader,\r\n",
        "    loss_fn, \r\n",
        "    device, \r\n",
        "    len(X_val)\r\n",
        "  )\r\n",
        "\r\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\r\n",
        "  print()\r\n",
        "\r\n",
        "  history['train_acc'].append(train_acc)\r\n",
        "  history['train_loss'].append(train_loss)\r\n",
        "  history['val_acc'].append(val_acc)\r\n",
        "  history['val_loss'].append(val_loss)\r\n",
        "\r\n",
        "  if val_acc > best_accuracy:\r\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\r\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 0.38272206365553185 accuracy 0.882884195193008\n",
            "Val   loss 0.25773550960002467 accuracy 0.931758530183727\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.15707014350797224 accuracy 0.9613983976693372\n",
            "Val   loss 0.30218571800893795 accuracy 0.9343832020997376\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.0778494997832621 accuracy 0.9809176984705025\n",
            "Val   loss 0.2598658752855651 accuracy 0.94750656167979\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.040385066958274256 accuracy 0.9905316824471959\n",
            "Val   loss 0.3027964792478694 accuracy 0.9396325459317585\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.019400862002167254 accuracy 0.9947560087399854\n",
            "Val   loss 0.26633608057090896 accuracy 0.9553805774278215\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.01204925697176043 accuracy 0.9966496722505462\n",
            "Val   loss 0.26926551365613705 accuracy 0.9553805774278215\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.00988708442712587 accuracy 0.9959213401310997\n",
            "Val   loss 0.28030040720598964 accuracy 0.94750656167979\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.006346419133123438 accuracy 0.9972323379461033\n",
            "Val   loss 0.2780750200921223 accuracy 0.9553805774278215\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.006456575878516116 accuracy 0.9973780043699927\n",
            "Val   loss 0.29646270789756574 accuracy 0.9501312335958005\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.005907374186266003 accuracy 0.9973780043699927\n",
            "Val   loss 0.30194062579812453 accuracy 0.94750656167979\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21cKw-70S4GA"
      },
      "source": [
        "## Proses Pengujian\r\n",
        "Mengembalikan terlebih dahulu hasil data test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXFUk1vHCmXj",
        "outputId": "d2c87488-4d18-4ad9-d935-2e1cbd8d5f82"
      },
      "source": [
        "test_acc, _ = eval_model(\r\n",
        "  model,\r\n",
        "  test_data_loader,\r\n",
        "  loss_fn,\r\n",
        "  device,\r\n",
        "  len(X_test)\r\n",
        ")\r\n",
        "\r\n",
        "test_acc.item()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9345549738219896"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKfur_DTTDEr"
      },
      "source": [
        "Membuat fungsi untuk mengembalikan label hasil prediksi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1w6aODzImx5"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "def get_predictions(model, data_loader):\r\n",
        "  model = model.eval()\r\n",
        "  \r\n",
        "  story_texts = []\r\n",
        "  predictions = []\r\n",
        "  prediction_probs = []\r\n",
        "  real_values = []\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for d in data_loader:\r\n",
        "\r\n",
        "      texts = d[\"story_text\"]\r\n",
        "      input_ids = d[\"input_ids\"].to(device)\r\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "      targets = d[\"targets\"].to(device)\r\n",
        "\r\n",
        "      outputs = model(\r\n",
        "        input_ids=input_ids,\r\n",
        "        attention_mask=attention_mask\r\n",
        "      )\r\n",
        "      _, preds = torch.max(outputs, dim=1)\r\n",
        "\r\n",
        "      probs = F.softmax(outputs, dim=1)\r\n",
        "\r\n",
        "      story_texts.extend(texts)\r\n",
        "      predictions.extend(preds)\r\n",
        "      prediction_probs.extend(probs)\r\n",
        "      real_values.extend(targets)\r\n",
        "\r\n",
        "  predictions = torch.stack(predictions).cpu()\r\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\r\n",
        "  real_values = torch.stack(real_values).cpu()\r\n",
        "  return story_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n1kh8AmTIwZ"
      },
      "source": [
        "Mengoutputkan classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR904tMhJTcZ"
      },
      "source": [
        "y_story_texts, y_pred, y_pred_probs, y_test = get_predictions(\r\n",
        "  model,\r\n",
        "  test_data_loader\r\n",
        ")"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90_Yl7CpJUNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82c5bba-63fc-4509-cf7e-bb7ad7b1dbdc"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.88      0.90        82\n",
            "           1       0.96      0.95      0.96       149\n",
            "           2       0.95      0.96      0.95        96\n",
            "           3       0.86      0.93      0.89        55\n",
            "\n",
            "    accuracy                           0.93       382\n",
            "   macro avg       0.92      0.93      0.93       382\n",
            "weighted avg       0.94      0.93      0.93       382\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}